{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA36gue6YuBfKt6jgXEyBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franciscorigon/chatbot_gemini/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando o SDK do Google"
      ],
      "metadata": {
        "id": "d_7zuPZyHCCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai\n",
        "\n",
        "# Importando\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('secret_key')\n",
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "5zB3zhxGDSRR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listar modelos disponíveis"
      ],
      "metadata": {
        "id": "BiquOxM8HMUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(f\"{m.temperature} - {m.name} - {m.description}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "gLB8lUwGE4zK",
        "outputId": "6b43395a-c78c-480d-f266-8d15cc8b3eaa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9 - models/gemini-1.0-pro - The best model for scaling across a wide range of tasks\n",
            "0.9 - models/gemini-1.0-pro-001 - The best model for scaling across a wide range of tasks. This is a stable model that supports tuning.\n",
            "0.9 - models/gemini-1.0-pro-latest - The best model for scaling across a wide range of tasks. This is the latest model.\n",
            "0.4 - models/gemini-1.0-pro-vision-latest - The best image understanding model to handle a broad range of applications\n",
            "1.0 - models/gemini-1.5-pro-latest - Mid-size multimodal model that supports up to 1 million tokens\n",
            "0.9 - models/gemini-pro - The best model for scaling across a wide range of tasks\n",
            "0.4 - models/gemini-pro-vision - The best image understanding model to handle a broad range of applications\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar variável pque configura o modelo"
      ],
      "metadata": {
        "id": "jbkBx1ZYKRrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "    \"candidate_count\": 1,\n",
        "    \"temperature\": 0.5,\n",
        "}"
      ],
      "metadata": {
        "id": "iSSDbaHkKcC8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurações de segurança\n"
      ],
      "metadata": {
        "id": "VLKiaocbLOhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "safety_settings = {\n",
        "    \"HARASSMENT\": \"BLOCK_NONE\",\n",
        "    \"HATE\": \"BLOCK_NONE\",\n",
        "    \"SEXUAL\": \"BLOCK_NONE\",\n",
        "    \"DANGEROUS\": \"BLOCK_NONE\",\n",
        "}"
      ],
      "metadata": {
        "id": "T55Blb7SLLoC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializando o modelo"
      ],
      "metadata": {
        "id": "bRSQfj9WLxRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
        "                              generation_config=generation_config,\n",
        "                              safety_settings=safety_settings)"
      ],
      "metadata": {
        "id": "XgVqf7D8LwmN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializando o modelo"
      ],
      "metadata": {
        "id": "pfv3369-NC8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"10 animais brasileiros\")\n",
        "tokens_number = model.count_tokens\n",
        "\n",
        "# print(f'Número de tokens: {tokens_number}')\n",
        "print(f'Claro Francisco, aqui está sua resposta:\\n\\n{response.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "Z9Cs7EcHNCE6",
        "outputId": "52ea4fa7-b7e6-483d-8f70-68e874de5dfc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claro Francisco, aqui está sua resposta:\n",
            "\n",
            "1. Onça-pintada\n",
            "2. Arara-azul\n",
            "3. Macaco-prego\n",
            "4. Capivara\n",
            "5. Anta\n",
            "6. Lobo-guará\n",
            "7. Tamanduá-bandeira\n",
            "8. Preguiça\n",
            "9. Tucano\n",
            "10. Piranha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat bot"
      ],
      "metadata": {
        "id": "XC3MGVLAQlsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(history=[])\n",
        "\n",
        "prompt = input(\"Escreva aqui: \")\n",
        "\n",
        "while prompt != \"fim\":\n",
        "  response = chat.send_message(prompt)\n",
        "  print(\"Resposta:\\n\\n\", response.text, \"\\n\")\n",
        "  prompt = input(\"Escreva aqui: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lDl3Nds4Np_P",
        "outputId": "ac08ac4f-34f2-4b40-f29c-4d8986e873d6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-3a7c56a86e02>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Escreva aqui: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"fim\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = chat.history"
      ],
      "metadata": {
        "id": "gsu67YFYUnTS"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Melhorando a visualização\n",
        "\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  \"\"\"Converte texto simples para Markdown para melhor formatação.\"\"\"\n",
        "  text = text.replace('`', r'\\`')  # Escapa caracteres de crase\n",
        "  return Markdown(textwrap.indent(text, '  '))\n",
        "\n",
        "# Imprimindo o histórico\n",
        "for message in chat_history:\n",
        "  display(to_markdown(f\"**{message.role}**: {message.parts[0].text}\"))\n",
        "  print(\"-\" * 135)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "XIPGgeqLT--L",
        "outputId": "10d25bc2-df75-4c4a-b612-d2393339c619"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "  **user**: Olá"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "  **model**: Olá, como posso ajudá-lo hoje?"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "  **user**: Estou precisando fazer meu almoço, me dê 5 sugestões"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "  **model**: **5 Sugestões de Almoço:**\n\n  1. **Salada de quinoa com grão de bico, vegetais assados e vinagrete de limão:** Quinoa cozida, grão de bico, vegetais assados (como brócolis, cenoura e cebola), vinagrete de limão, sal e pimenta.\n  2. **Sanduíche de atum com salada de repolho:** Atum em lata, maionese, mostarda, cebola picada, aipo picado, salada de repolho, pão integral.\n  3. **Sopa de legumes com pão crocante:** Legumes variados (como cenoura, aipo, cebola e batata), caldo de legumes, ervas e especiarias, pão crocante para acompanhar.\n  4. **Wrap de frango grelhado com hummus e vegetais:** Frango grelhado fatiado, hummus, vegetais (como alface, tomate e pepino), pão sírio integral.\n  5. **Macarrão integral com molho de tomate e queijo:** Macarrão integral cozido, molho de tomate, queijo parmesão ralado, manjericão fresco."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FwVjRcbmUCL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}